{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "***\n",
    "\n",
    "# A2 Data Analysis and Code | Classification Modeling\n",
    "**Machine Learning**<br />\n",
    "Shresth Sethi - Student MSBA<br />\n",
    "FMSBA5 - Valencia<br />\n",
    "Hult International Business School<br><br>\n",
    "\n",
    "***\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "**PROJECT APPRENTICE CHEF - Halfway There Cross-Sell service**<br /><br />\n",
    "**AIM**: Of this project is to build a machine learning model to predict weather a customer will subscribe to Halfway There service.<br />\n",
    "\n",
    "**BACKGROUND**: Halfway There is a unique subscription in which subscribers will get half bottle of wine from local California vineyard every Wednesday.<br />\n",
    "\n",
    "**WHY**: Apprentice Chef want to diversify there revenue stream by adding a cross-sell service. Plus this will give Apprentice Chef competitive advantage based on its unique product offering of hard to find local wines.<br />\n",
    "\n",
    "**ASSUMPTION**: The dataset provided by the engineering team has used dataset engineering techniques and are statistically sound and represent the true picture of Apprentice Chefâ€™s customers.<br />\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the required libraries\n",
    "import pandas as pd                                      # data science essentials\n",
    "import matplotlib.pyplot as plt                          # data visualization\n",
    "import seaborn as sns                                    # enhanced data visualization\n",
    "import statsmodels.formula.api as smf                    # regression modeling\n",
    "from sklearn.model_selection import train_test_split     # train/test split\n",
    "import sklearn.linear_model                              # linear models\n",
    "from sklearn.neighbors import KNeighborsRegressor        # KNN for Regression\n",
    "from sklearn.preprocessing import StandardScaler         # standard scaler\n",
    "from sklearn.metrics import confusion_matrix             # confusion matrix\n",
    "from sklearn.metrics import roc_auc_score                # auc score\n",
    "from sklearn.neighbors import KNeighborsClassifier       # KNN for classification\n",
    "from sklearn.tree import DecisionTreeClassifier          # classification trees\n",
    "from sklearn.tree import export_graphviz                 # exports graphics\n",
    "from sklearn.externals.six import StringIO               # saves objects in memory\n",
    "from IPython.display import Image                        # displays on frontend\n",
    "import pydotplus                                         # interprets dot objects\n",
    "from sklearn.model_selection import GridSearchCV         # hyperparameter tuning\n",
    "from sklearn.metrics import make_scorer                  # customizable scorer\n",
    "from sklearn.ensemble import RandomForestClassifier      # random forest\n",
    "from sklearn.ensemble import GradientBoostingClassifier  # gbm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading dataset to the environment**\n",
    "\n",
    "Doing this step to load the dataset and further perform analysis on it. I have also set the display option to see appropriate results from data loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data to the python file\n",
    "original_df = pd.read_excel(\"Apprentice_Chef_Dataset.xlsx\")\n",
    "original_description = pd.read_excel('Apprentice_Chef_Data_Dictionary.xlsx')\n",
    "\n",
    "#Setting print operations for df\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CHECKING MISSING VALUES**<br /> \n",
    "If any in the dataset<br /><br />\n",
    "\n",
    "***\n",
    "Found that the dataset is almost clean only the **FAMILY_NAME** column has some missing values.\n",
    "***\n",
    "\n",
    "Next I have dropped the FAMILY_NAME, NAME, and FIRST_NAME from the analysis as they do not add value to the steps performer further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking missing values and then summing it up to know the total column wise\n",
    "d  = original_df.isnull().sum()\n",
    "\n",
    "#as only FAMILY_NAME column has missing values\n",
    "#dropping FAMILY_NAME, NAME, FIRST_NAME\n",
    "\n",
    "original_df = original_df.drop(labels = ['FAMILY_NAME', 'NAME', 'FIRST_NAME'],\n",
    "                               axis   = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## **Exploratory Data Analysis**\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Categorical Data <br />\n",
    "Separating emails into different domains professional/ personal, and junk <br />\n",
    "This is done because the case provide different categories assigned to domain id. Further will be one hot encoding the group formed.\n",
    "\n",
    "STEP 1 : Separating the ID and Domain in different columns and storing it in email_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an empty list\n",
    "empty_lst = []\n",
    "\n",
    "\n",
    "\n",
    "# looping over each email address\n",
    "for index, col in original_df.iterrows():\n",
    "    \n",
    "    # splitting email domain at '@'\n",
    "    split_email = original_df.loc[index, 'EMAIL'].split(sep = '@')\n",
    "    \n",
    "    # appending the list\n",
    "    empty_lst.append(split_email)\n",
    "    \n",
    "\n",
    "\n",
    "# converting empty_lst into a DataFrame \n",
    "email_df = pd.DataFrame(empty_lst)\n",
    "\n",
    "\n",
    "\n",
    "# Creating the id and domain column in email_df\n",
    "email_df.columns = ['ID', 'DOMAIN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 2 : Joining the data of email_df with original_df<br />\n",
    "    \n",
    "*NOTE : I will only be using domain to categorize so the join will be with only the DOMAIN column of email_df*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = pd.concat([  original_df, \n",
    "                           email_df.loc[:,'DOMAIN']], \n",
    "                           axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 3 : Creating customer email domain groups\n",
    "\n",
    "*Professional/Personal/Junk* these are the categories that case defined, now assigning these with the values of domain id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list with professional and assigning domain id related to it\n",
    "professional = ['@mmm.com', '@amex.com', '@apple.com','@boeing.com',\n",
    "                '@caterpillar.com', '@chevron.com', '@cisco.com',\n",
    "                '@cocacola.com','@disney.com', '@dupont.com',\n",
    "                '@exxon.com', '@ge.org','@goldmansacs.com', \n",
    "                '@homedepot.com', '@ibm.com', '@intel.com',\n",
    "                '@jnj.com', '@jpmorgan.com', '@mcdonalds.com',\n",
    "                '@merck.com', '@microsoft.com', '@nike.com', \n",
    "                '@pfizer.com', '@pg.com', '@travelers.com',\n",
    "                '@unitedtech.com', '@unitedhealth.com', \n",
    "                '@verizon.com', '@visa.com', '@walmart.com']\n",
    "\n",
    "\n",
    "#Creating a list with personal and assigning domain id related to it\n",
    "personal     = ['@gmail.com', '@yahoo.com', '@protonmail.com']\n",
    "\n",
    "\n",
    "#Creating a list with junk and assigning domain id related to it\n",
    "junk         = ['@me.com', '@aol.com', '@hotmail.com', '@live.com',\n",
    "                '@msn.com', '@passport.com']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 4 : Assigning the groups created for email domains\n",
    "\n",
    "Storing the values by creating a new column 'DOMAIN_GRP' in the df for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an empty list\n",
    "empty_lst = []\n",
    "\n",
    "\n",
    "\n",
    "# looping to assign domains\n",
    "for i in original_df['DOMAIN']:\n",
    "    \n",
    "        if   '@'+ i in professional:\n",
    "             empty_lst.append('professional') #professional emails\n",
    "            \n",
    "        elif '@' + i in personal:\n",
    "             empty_lst.append('personal') # personal emails\n",
    "\n",
    "        elif '@' + i in junk:\n",
    "             empty_lst.append('junk') # junk emails\n",
    "            \n",
    "        else:\n",
    "             print('Unknown')\n",
    "\n",
    "                \n",
    "\n",
    "# Creating a new column in the original dataset to store the assigned values\n",
    "original_df['DOMAIN_GRP'] = pd.DataFrame(empty_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 5 : Creating one hot encoding variables\n",
    "\n",
    "Dropping the \"DOMAIN_GRP\",\"DOMAIN\", and \"EMAIL\" column as it has already been encoded and then \"domain one hot\" data with original data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using get_dummies to encode the DOMAIN_GRP column\n",
    "domain_one_hot = pd.get_dummies(original_df['DOMAIN_GRP'])\n",
    "\n",
    "\n",
    "\n",
    "#Dropping the columns\n",
    "original_df          = original_df.drop(['DOMAIN_GRP', 'DOMAIN', 'EMAIL'], axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "#Joining the one hot encoding with the df\n",
    "original_df          = original_df.join([domain_one_hot])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint 1**\n",
    "\n",
    "Creating a check point to save the data, doing this to not change the original EXCEL data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving results\n",
    "original_df.to_excel('chef_feature_rich.xlsx',\n",
    "                 index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading saved file\n",
    "original_df = pd.read_excel('chef_feature_rich.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Outliers<br />\n",
    "**Visualizing numerical data**\n",
    "\n",
    "Doing this by using boxplots and distplot, this is done to identify the outliers so that it can be treated separately for the analysis. Checking outliers and creating different columns for them and seeing how it effects the results. Visual are required to base the cutoff for different variables or set values to certain variables.\n",
    "\n",
    "\n",
    "*I have commented out this code for faster processing though have based my decisions referring to these graph.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#creating subplots for better visuals\n",
    "\n",
    "#fig, ax = plt.subplots(figsize = (10, 8)) \n",
    "#plt.subplot(2, 2, 1)\n",
    "#sns.boxplot(original_df['TOTAL_MEALS_ORDERED'])\n",
    "#plt.subplot(2, 2, 2)\n",
    "#sns.distplot(original_df['UNIQUE_MEALS_PURCH'])\n",
    "#plt.subplot(2, 2, 3)\n",
    "#sns.boxplot(original_df['CONTACTS_W_CUSTOMER_SERVICE'])\n",
    "#plt.subplot(2, 2, 4)\n",
    "#sns.boxplot(original_df['PRODUCT_CATEGORIES_VIEWED'])\n",
    "\n",
    "#fig, ax = plt.subplots(figsize = (10, 8))\n",
    "#plt.subplot(2, 2, 1)\n",
    "#sns.boxplot(original_df['AVG_TIME_PER_SITE_VISIT'])\n",
    "#plt.subplot(2, 2, 2)\n",
    "#sns.distplot(original_df['MOBILE_NUMBER'])\n",
    "#plt.subplot(2, 2, 3)\n",
    "#sns.boxplot(original_df['CANCELLATIONS_BEFORE_NOON'])\n",
    "#plt.subplot(2, 2, 4)\n",
    "#sns.boxplot(original_df['CANCELLATIONS_AFTER_NOON'])\n",
    "\n",
    "#fig, ax = plt.subplots(figsize = (10, 8))\n",
    "#plt.subplot(2, 2, 1)\n",
    "#sns.distplot(original_df['TASTES_AND_PREFERENCES'])\n",
    "#plt.subplot(2,2,2)\n",
    "#sns.distplot(original_df['PC_LOGINS'])\n",
    "#plt.subplot(2,2,3)\n",
    "#sns.distplot(original_df['MOBILE_LOGINS'])\n",
    "#plt.subplot(2,2,4)\n",
    "#sns.boxplot(original_df['WEEKLY_PLAN'])\n",
    "\n",
    "#fig, ax = plt.subplots(figsize = (10, 8))\n",
    "#plt.subplot(2, 2, 1)\n",
    "#sns.distplot(original_df['EARLY_DELIVERIES'])\n",
    "#plt.subplot(2,2,2)\n",
    "#sns.boxplot(original_df['LATE_DELIVERIES'])\n",
    "#plt.subplot(2,2,3)\n",
    "#sns.distplot(original_df['PACKAGE_LOCKER'])\n",
    "#plt.subplot(2,2,4)\n",
    "#sns.distplot(original_df['REFRIGERATED_LOCKER'])\n",
    "\n",
    "#fig, ax = plt.subplots(figsize = (10, 8))\n",
    "#plt.subplot(2, 2, 1)\n",
    "#sns.boxplot(original_df['FOLLOWED_RECOMMENDATIONS_PCT'])\n",
    "#plt.subplot(2,2,2)\n",
    "#sns.boxplot(original_df['AVG_PREP_VID_TIME'])\n",
    "#plt.subplot(2,2,3)\n",
    "#sns.boxplot(original_df['LARGEST_ORDER_SIZE'])\n",
    "#plt.subplot(2,2,4)\n",
    "#sns.distplot(original_df['MASTER_CLASSES_ATTENDED'])\n",
    "\n",
    "#fig, ax = plt.subplots(figsize = (10, 8))\n",
    "#plt.subplot(2, 2, 1)\n",
    "#sns.distplot(original_df['MEDIAN_MEAL_RATING'])\n",
    "#plt.subplot(2,2,2)\n",
    "#sns.boxplot(original_df['AVG_CLICKS_PER_VISIT'])\n",
    "#plt.subplot(2,2,3)\n",
    "#sns.distplot(original_df['TOTAL_PHOTOS_VIEWED'])\n",
    "#plt.subplot(2,2,4)\n",
    "#sns.distplot(original_df['junk'])\n",
    "\n",
    "#fig, ax = plt.subplots(figsize = (10, 8))\n",
    "#plt.subplot(2, 2, 1)\n",
    "#sns.distplot(original_df['personal'])\n",
    "#plt.subplot(2,2,2)\n",
    "#sns.distplot(original_df['professional'])\n",
    "#plt.subplot(2,2,3)\n",
    "#sns.distplot(original_df['CROSS_SELL_SUCCESS'])\n",
    "#plt.subplot(2,2,4)\n",
    "#sns.boxplot(original_df['REVENUE'])\n",
    "\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## **Feature Engineering**\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining Thresholds**\n",
    "\n",
    "Have used the above graphs to define thresholds for different variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting Thresholds for outliers\n",
    "\n",
    "REVENUE_out = 4200 \n",
    "TOTAL_MEALS_ORDERED_out = 180\n",
    "UNIQUE_MEALS_PURCH_out = 8\n",
    "CONTACTS_W_CUSTOMER_SERVICE_hi = 8\n",
    "CONTACTS_W_CUSTOMER_SERVICE_lo = 5\n",
    "PRODUCT_CATEGORIES_VIEWED_at = 5\n",
    "AVG_TIME_PER_SITE_VISIT_out = 200\n",
    "MOBILE_NUMBER_at = 1\n",
    "CANCELLATIONS_BEFORE_NOON_at = 1\n",
    "CANCELLATIONS_AFTER_NOON_at = 0\n",
    "TASTES_AND_PREFERENCES_at = 1\n",
    "PC_LOGINS_at = 5\n",
    "PC_LOGINS_at1 = 6\n",
    "MOBILE_LOGINS_at = 1\n",
    "MOBILE_LOGINS_at1 = 2\n",
    "WEEKLY_PLAN_out = 30\n",
    "EARLY_DELIVERIES_at = 0\n",
    "LATE_DELIVERIES_at = 3\n",
    "PACKAGE_LOCKER_at = 0\n",
    "PACKAGE_LOCKER_at1 = 1\n",
    "REFRIGERATED_LOCKER_at = 0\n",
    "FOLLOWED_RECOMMENDATIONS_PCT_at = 30\n",
    "AVG_PREP_VID_TIME_out = 280\n",
    "LARGEST_ORDER_SIZE_at = 4\n",
    "MASTER_CLASSES_ATTENDED_at = 0\n",
    "MASTER_CLASSES_ATTENDED_at1 = 1\n",
    "MEDIAN_MEAL_RATING_at = 3\n",
    "AVG_CLICKS_PER_VISIT_at = 13\n",
    "TOTAL_PHOTOS_VIEWED_at = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new outliers variable\n",
    "\n",
    "Based on the threshold I have created following outliers variables for final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting REVENUE outliers\n",
    "original_df['REVENUE_out'] = 0\n",
    "condition_hi = original_df.loc[0:,'REVENUE_out'][original_df['REVENUE'] > REVENUE_out]\n",
    "original_df['REVENUE_out'].replace(to_replace = condition_hi,\n",
    "                                                   value      = 1,\n",
    "                                                   inplace    = True)\n",
    "\n",
    "# Selecting TOTAL_MEALS_ORDERED_out outliers\n",
    "original_df['TOTAL_MEALS_ORDERED_out'] = 0\n",
    "condition_hi = original_df.loc[0:,'TOTAL_MEALS_ORDERED_out'][original_df['TOTAL_MEALS_ORDERED'] > TOTAL_MEALS_ORDERED_out]\n",
    "original_df['TOTAL_MEALS_ORDERED_out'].replace(to_replace = condition_hi,\n",
    "                                                   value      = 1,\n",
    "                                                   inplace    = True)\n",
    "#Selecting UNIQUE_MEALS_PURCH_out outliers\n",
    "original_df['UNIQUE_MEALS_PURCH_out'] = 0\n",
    "condition_hi = original_df.loc[0:,'UNIQUE_MEALS_PURCH_out'][original_df['UNIQUE_MEALS_PURCH'] > UNIQUE_MEALS_PURCH_out]\n",
    "original_df['UNIQUE_MEALS_PURCH_out'].replace(to_replace = condition_hi,\n",
    "                                                   value      = 1,\n",
    "                                                   inplace    = True)\n",
    "\n",
    "#Selecting CONTACTS_W_CUSTOMER_SERVICE_out defining boundries for outliers\n",
    "original_df['CONTACTS_W_CUSTOMER_SERVICE_out'] = 0\n",
    "condition_hi = original_df.loc[0:,'CONTACTS_W_CUSTOMER_SERVICE_out'][original_df['CONTACTS_W_CUSTOMER_SERVICE'] > CONTACTS_W_CUSTOMER_SERVICE_hi]\n",
    "condition_lo = original_df.loc[0:,'CONTACTS_W_CUSTOMER_SERVICE_out'][original_df['CONTACTS_W_CUSTOMER_SERVICE'] < CONTACTS_W_CUSTOMER_SERVICE_lo]\n",
    "original_df['CONTACTS_W_CUSTOMER_SERVICE_out'].replace(to_replace = condition_hi,\n",
    "                                                   value      = 1,\n",
    "                                                   inplace    = True)\n",
    "original_df['CONTACTS_W_CUSTOMER_SERVICE_out'].replace(to_replace = condition_lo,\n",
    "                                              value      = 1,\n",
    "                                              inplace    = True)\n",
    "\n",
    "#Selecting PRODUCT_CATEGORIES_VIEWED_at\n",
    "original_df['PRODUCT_CATEGORIES_VIEWED_at'] = 0\n",
    "condition_hi = original_df.loc[0:,'PRODUCT_CATEGORIES_VIEWED_at'][original_df['PRODUCT_CATEGORIES_VIEWED'] == PRODUCT_CATEGORIES_VIEWED_at]\n",
    "original_df['PRODUCT_CATEGORIES_VIEWED_at'].replace(to_replace = condition_hi,\n",
    "                                                   value      = 1,\n",
    "                                                   inplace    = True)\n",
    "\n",
    "#Selecting AVG_TIME_PER_SITE_VISIT_out\n",
    "original_df['AVG_TIME_PER_SITE_VISIT_out'] = 0\n",
    "condition_hi = original_df.loc[0:,'AVG_TIME_PER_SITE_VISIT_out'][original_df['AVG_TIME_PER_SITE_VISIT'] > AVG_TIME_PER_SITE_VISIT_out]\n",
    "original_df['AVG_TIME_PER_SITE_VISIT_out'].replace(to_replace = condition_hi,\n",
    "                                                   value      = 1,\n",
    "                                                   inplace    = True)\n",
    "\n",
    "#Selecting MOBILE_NUMBER_at\n",
    "original_df['MOBILE_NUMBER_at'] = 0\n",
    "condition_hi = original_df.loc[0:,'MOBILE_NUMBER_at'][original_df['MOBILE_NUMBER'] == MOBILE_NUMBER_at]\n",
    "original_df['MOBILE_NUMBER_at'].replace(to_replace = condition_hi,\n",
    "                                                   value      = 1,\n",
    "                                                   inplace    = True)\n",
    "\n",
    "#Selecting CANCELLATIONS_BEFORE_NOON_at\n",
    "original_df['CANCELLATIONS_BEFORE_NOON_at'] = 0\n",
    "condition_hi = original_df.loc[0:,'CANCELLATIONS_BEFORE_NOON_at'][original_df['CANCELLATIONS_BEFORE_NOON'] == CANCELLATIONS_BEFORE_NOON_at]\n",
    "original_df['CANCELLATIONS_BEFORE_NOON_at'].replace(to_replace = condition_hi,\n",
    "                                                   value      = 1,\n",
    "                                                   inplace    = True)\n",
    "\n",
    "#Selecting CANCELLATIONS_AFTER_NOON_at\n",
    "original_df['CANCELLATIONS_AFTER_NOON_at'] = 0\n",
    "condition_hi = original_df.loc[0:,'CANCELLATIONS_AFTER_NOON_at'][original_df['CANCELLATIONS_AFTER_NOON'] == CANCELLATIONS_AFTER_NOON_at]\n",
    "original_df['CANCELLATIONS_AFTER_NOON_at'].replace(to_replace = condition_hi,\n",
    "                                                   value      = 1,\n",
    "                                                   inplace    = True)\n",
    "\n",
    "#Selecting TASTES_AND_PREFERENCES_at\n",
    "original_df['TASTES_AND_PREFERENCES_at'] = 0\n",
    "condition_hi = original_df.loc[0:,'TASTES_AND_PREFERENCES_at'][original_df['TASTES_AND_PREFERENCES'] == TASTES_AND_PREFERENCES_at]\n",
    "original_df['TASTES_AND_PREFERENCES_at'].replace(to_replace = condition_hi,\n",
    "                                                   value      = 1,\n",
    "                                                   inplace    = True)\n",
    "\n",
    "#Selecting PC_LOGINS_at\n",
    "original_df['PC_LOGINS_at'] = 0\n",
    "condition_hi = original_df.loc[0:,'PC_LOGINS_at'][original_df['PC_LOGINS'] == PC_LOGINS_at]\n",
    "original_df['PC_LOGINS_at'].replace(to_replace = condition_hi,\n",
    "                                                   value      = 1,\n",
    "                                                   inplace    = True)\n",
    "\n",
    "#Selecting PC_LOGINS_at1\n",
    "original_df['PC_LOGINS_at1'] = 0\n",
    "condition_hi = original_df.loc[0:,'PC_LOGINS_at1'][original_df['PC_LOGINS'] == PC_LOGINS_at1]\n",
    "original_df['PC_LOGINS_at1'].replace(to_replace = condition_hi,\n",
    "                                                   value      = 1,\n",
    "                                                   inplace    = True)\n",
    "\n",
    "#Selecting MOBILE_LOGINS_at\n",
    "original_df['MOBILE_LOGINS_at'] = 0\n",
    "condition_hi = original_df.loc[0:,'MOBILE_LOGINS_at'][original_df['MOBILE_LOGINS'] == MOBILE_LOGINS_at]\n",
    "original_df['MOBILE_LOGINS_at'].replace(to_replace = condition_hi,\n",
    "                                                   value      = 1,\n",
    "                                                   inplace    = True)\n",
    "\n",
    "#Selecting MOBILE_LOGINS_at1\n",
    "original_df['MOBILE_LOGINS_at1'] = 0\n",
    "condition_hi = original_df.loc[0:,'MOBILE_LOGINS_at1'][original_df['MOBILE_LOGINS'] == MOBILE_LOGINS_at1]\n",
    "original_df['MOBILE_LOGINS_at1'].replace(to_replace = condition_hi,\n",
    "                                                   value      = 1,\n",
    "                                                   inplace    = True)\n",
    "\n",
    "#Selecting WEEKLY_PLAN_out\n",
    "original_df['WEEKLY_PLAN_out'] = 0\n",
    "condition_hi = original_df.loc[0:,'WEEKLY_PLAN_out'][original_df['WEEKLY_PLAN'] > WEEKLY_PLAN_out]\n",
    "original_df['WEEKLY_PLAN_out'].replace(to_replace = condition_hi,\n",
    "                                                   value      = 1,\n",
    "                                                   inplace    = True)\n",
    "\n",
    "#Selecting EARLY_DELIVERIES_at\n",
    "original_df['EARLY_DELIVERIES_at'] = 0\n",
    "condition_hi = original_df.loc[0:,'EARLY_DELIVERIES_at'][original_df['EARLY_DELIVERIES'] == EARLY_DELIVERIES_at]\n",
    "original_df['EARLY_DELIVERIES_at'].replace(to_replace = condition_hi,\n",
    "                                                   value      = 1,\n",
    "                                                   inplace    = True)\n",
    "\n",
    "#Selecting LATE_DELIVERIES_at\n",
    "original_df['LATE_DELIVERIES_at'] = 0\n",
    "condition_hi = original_df.loc[0:,'LATE_DELIVERIES_at'][original_df['LATE_DELIVERIES'] == LATE_DELIVERIES_at]\n",
    "original_df['LATE_DELIVERIES_at'].replace(to_replace = condition_hi,\n",
    "                                                   value      = 1,\n",
    "                                                   inplace    = True)\n",
    "\n",
    "#Selecting PACKAGE_LOCKER_at\n",
    "original_df['PACKAGE_LOCKER_at'] = 0\n",
    "condition_hi = original_df.loc[0:,'PACKAGE_LOCKER_at'][original_df['PACKAGE_LOCKER'] == PACKAGE_LOCKER_at]\n",
    "original_df['PACKAGE_LOCKER_at'].replace(to_replace = condition_hi,\n",
    "                                                   value      = 1,\n",
    "                                                   inplace    = True)\n",
    "\n",
    "#Selecting PACKAGE_LOCKER_at1\n",
    "original_df['PACKAGE_LOCKER_at1'] = 0\n",
    "condition_hi = original_df.loc[0:,'PACKAGE_LOCKER_at1'][original_df['PACKAGE_LOCKER'] == PACKAGE_LOCKER_at1]\n",
    "original_df['PACKAGE_LOCKER_at1'].replace(to_replace = condition_hi,\n",
    "                                                   value      = 1,\n",
    "                                                   inplace    = True)\n",
    "\n",
    "#Selecting REFRIGERATED_LOCKER_at\n",
    "original_df['REFRIGERATED_LOCKER_at'] = 0\n",
    "condition_hi = original_df.loc[0:,'REFRIGERATED_LOCKER_at'][original_df['REFRIGERATED_LOCKER'] == REFRIGERATED_LOCKER_at]\n",
    "original_df['REFRIGERATED_LOCKER_at'].replace(to_replace = condition_hi,\n",
    "                                                   value      = 1,\n",
    "                                                   inplace    = True)\n",
    "\n",
    "#Selecting FOLLOWED_RECOMMENDATIONS_PCT_at\n",
    "original_df['FOLLOWED_RECOMMENDATIONS_PCT_at'] = 0\n",
    "condition_hi = original_df.loc[0:,'FOLLOWED_RECOMMENDATIONS_PCT_at'][original_df['FOLLOWED_RECOMMENDATIONS_PCT'] == FOLLOWED_RECOMMENDATIONS_PCT_at]\n",
    "original_df['FOLLOWED_RECOMMENDATIONS_PCT_at'].replace(to_replace = condition_hi,\n",
    "                                                   value      = 1,\n",
    "                                                   inplace    = True)\n",
    "\n",
    "\n",
    "#Selecting AVG_PREP_VID_TIME_out\n",
    "original_df['AVG_PREP_VID_TIME_out'] = 0\n",
    "condition_hi = original_df.loc[0:,'AVG_PREP_VID_TIME_out'][original_df['AVG_PREP_VID_TIME'] > AVG_PREP_VID_TIME_out]\n",
    "original_df['AVG_PREP_VID_TIME_out'].replace(to_replace = condition_hi,\n",
    "                                                   value      = 1,\n",
    "                                                   inplace    = True)\n",
    "\n",
    "#Selecting LARGEST_ORDER_SIZE_at\n",
    "original_df['LARGEST_ORDER_SIZE_at'] = 0\n",
    "condition_hi = original_df.loc[0:,'LARGEST_ORDER_SIZE_at'][original_df['LARGEST_ORDER_SIZE'] == LARGEST_ORDER_SIZE_at]\n",
    "original_df['LARGEST_ORDER_SIZE_at'].replace(to_replace = condition_hi,\n",
    "                                                   value      = 1,\n",
    "                                                   inplace    = True)\n",
    "\n",
    "#Selecting MASTER_CLASSES_ATTENDED_at\n",
    "original_df['MASTER_CLASSES_ATTENDED_at'] = 0\n",
    "condition_hi = original_df.loc[0:,'MASTER_CLASSES_ATTENDED_at'][original_df['MASTER_CLASSES_ATTENDED'] == MASTER_CLASSES_ATTENDED_at]\n",
    "original_df['MASTER_CLASSES_ATTENDED_at'].replace(to_replace = condition_hi,\n",
    "                                                   value      = 1,\n",
    "                                                   inplace    = True)\n",
    "\n",
    "#Selecting MASTER_CLASSES_ATTENDED_at1\n",
    "original_df['MASTER_CLASSES_ATTENDED_at1'] = 0\n",
    "condition_hi = original_df.loc[0:,'MASTER_CLASSES_ATTENDED_at1'][original_df['MASTER_CLASSES_ATTENDED'] == MASTER_CLASSES_ATTENDED_at1]\n",
    "original_df['MASTER_CLASSES_ATTENDED_at1'].replace(to_replace = condition_hi,\n",
    "                                                   value      = 1,\n",
    "                                                   inplace    = True)\n",
    "\n",
    "#Selecting MEDIAN_MEAL_RATING_at\n",
    "original_df['MEDIAN_MEAL_RATING_at'] = 0\n",
    "condition_hi = original_df.loc[0:,'MEDIAN_MEAL_RATING_at'][original_df['MEDIAN_MEAL_RATING'] == MEDIAN_MEAL_RATING_at]\n",
    "original_df['MEDIAN_MEAL_RATING_at'].replace(to_replace = condition_hi,\n",
    "                                                   value      = 1,\n",
    "                                                   inplace    = True)\n",
    "\n",
    "#Selecting AVG_CLICKS_PER_VISIT_at\n",
    "original_df['AVG_CLICKS_PER_VISIT_at'] = 0\n",
    "condition_hi = original_df.loc[0:,'AVG_CLICKS_PER_VISIT_at'][original_df['AVG_CLICKS_PER_VISIT'] == AVG_CLICKS_PER_VISIT_at]\n",
    "original_df['AVG_CLICKS_PER_VISIT_at'].replace(to_replace = condition_hi,\n",
    "                                                   value      = 1,\n",
    "                                                   inplace    = True)\n",
    "#Selecting TOTAL_PHOTOS_VIEWED_at\n",
    "original_df['TOTAL_PHOTOS_VIEWED_at'] = 0\n",
    "condition_hi = original_df.loc[0:,'TOTAL_PHOTOS_VIEWED_at'][original_df['TOTAL_PHOTOS_VIEWED'] == TOTAL_PHOTOS_VIEWED_at]\n",
    "original_df['TOTAL_PHOTOS_VIEWED_at'].replace(to_replace = condition_hi,\n",
    "                                                   value      = 1,\n",
    "                                                   inplace    = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint 2**\n",
    "\n",
    "Creating a check point to save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving results\n",
    "original_df.to_excel('chef_feature_rich.xlsx',\n",
    "                 index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading saved file\n",
    "original_df = pd.read_excel('chef_feature_rich.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## **Modeling Techniques**\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "\n",
    "### CORRELATION MATRIX<br /> \n",
    "\n",
    "\n",
    "The correlations show by how much a value is related with revenue. These displayed below are all highly correlated variables. This mean any change in them will have positive/negative result on the model depending on the variable sign.\n",
    "\n",
    "**High Correlations**\n",
    "\n",
    "~~~\n",
    "CROSS_SELL_SUCCESS                 1.00\n",
    "FOLLOWED_RECOMMENDATIONS_PCT       0.46\n",
    "junk                              -0.28\n",
    "FOLLOWED_RECOMMENDATIONS_PCT_at   -0.24\n",
    "professional                       0.19\n",
    "CANCELLATIONS_BEFORE_NOON          0.16\n",
    "MOBILE_NUMBER                      0.10\n",
    "MOBILE_NUMBER_at                   0.10\n",
    "TASTES_AND_PREFERENCES_at          0.08\n",
    "TASTES_AND_PREFERENCES             0.08\n",
    "REFRIGERATED_LOCKER                0.07\n",
    "REFRIGERATED_LOCKER_at            -0.07\n",
    "~~~\n",
    "\n",
    "*The code below is commented out as the relevant results are displayed above.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating correlation for the dataset\n",
    "#df_corr = original_df.corr().round(2)\n",
    "\n",
    "\n",
    "#displaying result of correlation with respect to revenue\n",
    "#df_corr.loc[ : ,'CROSS_SELL_SUCCESS'].sort_values(ascending = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code was used to print out the column names with comma \",\" to use it in the next code.\n",
    "\n",
    "*Code has been commented to not display results as it is not relevant and faster processing.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing column names to use\n",
    "#for name in original_df.columns:\n",
    "#    print(f\"'{name}' ,\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Creating X variable with statistically significant columns**\n",
    "\n",
    "Printing the result with + sign to be used further in Logistic regression.\n",
    "\n",
    "- The print statement is commented as the statistically significant values are already pasted in the regression model<br />\n",
    "- Other commented values in the x_val are because they were not statistically significant and were removed by one after another depending on the p values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputing X variable and commenting out the statistically insignificant variables\n",
    "x_val = [   #'REVENUE' ,\n",
    "            #'TOTAL_MEALS_ORDERED' ,\n",
    "            #'UNIQUE_MEALS_PURCH' ,\n",
    "            #'CONTACTS_W_CUSTOMER_SERVICE' ,\n",
    "            #'PRODUCT_CATEGORIES_VIEWED' ,\n",
    "            #'AVG_TIME_PER_SITE_VISIT' ,\n",
    "            #'MOBILE_NUMBER' ,\n",
    "            'CANCELLATIONS_BEFORE_NOON' ,\n",
    "            #'CANCELLATIONS_AFTER_NOON' ,\n",
    "            #'TASTES_AND_PREFERENCES' ,\n",
    "            #'PC_LOGINS' ,\n",
    "            #'MOBILE_LOGINS' ,\n",
    "            #'WEEKLY_PLAN' ,\n",
    "            #'EARLY_DELIVERIES' ,\n",
    "            #'LATE_DELIVERIES' ,\n",
    "            #'PACKAGE_LOCKER' ,\n",
    "            #'REFRIGERATED_LOCKER' ,\n",
    "            'FOLLOWED_RECOMMENDATIONS_PCT' ,\n",
    "            #'AVG_PREP_VID_TIME' ,\n",
    "            #'LARGEST_ORDER_SIZE' ,\n",
    "            #'MASTER_CLASSES_ATTENDED' ,\n",
    "            #'MEDIAN_MEAL_RATING' ,\n",
    "            #'AVG_CLICKS_PER_VISIT' ,\n",
    "            #'TOTAL_PHOTOS_VIEWED' ,\n",
    "            #'junk' ,\n",
    "            'personal' ,\n",
    "            'professional' ,\n",
    "            #'REVENUE_out' ,\n",
    "            #'TOTAL_MEALS_ORDERED_out' ,\n",
    "            #'UNIQUE_MEALS_PURCH_out' ,\n",
    "            #'CONTACTS_W_CUSTOMER_SERVICE_out' ,\n",
    "            #'PRODUCT_CATEGORIES_VIEWED_at' ,\n",
    "            #'AVG_TIME_PER_SITE_VISIT_out' ,\n",
    "            'MOBILE_NUMBER_at' ,\n",
    "            #'CANCELLATIONS_BEFORE_NOON_at' ,\n",
    "            #'CANCELLATIONS_AFTER_NOON_at' ,\n",
    "            'TASTES_AND_PREFERENCES_at' ,\n",
    "            #'PC_LOGINS_at' ,\n",
    "            'PC_LOGINS_at1' ,\n",
    "            'MOBILE_LOGINS_at' ,\n",
    "            #'MOBILE_LOGINS_at1' ,\n",
    "            #'WEEKLY_PLAN_out' ,\n",
    "            #'EARLY_DELIVERIES_at' ,\n",
    "            #'LATE_DELIVERIES_at' ,\n",
    "            #'PACKAGE_LOCKER_at' ,\n",
    "            #'PACKAGE_LOCKER_at1' ,\n",
    "            #'REFRIGERATED_LOCKER_at' ,\n",
    "            'FOLLOWED_RECOMMENDATIONS_PCT_at' ,\n",
    "            #'AVG_PREP_VID_TIME_out' ,\n",
    "            #'LARGEST_ORDER_SIZE_at' ,\n",
    "            #'MASTER_CLASSES_ATTENDED_at' ,\n",
    "            #'MASTER_CLASSES_ATTENDED_at1' ,\n",
    "            #'MEDIAN_MEAL_RATING_at' ,\n",
    "            #'AVG_CLICKS_PER_VISIT_at' ,\n",
    "            #'TOTAL_PHOTOS_VIEWED_at'\n",
    "        ]\n",
    "\n",
    "#Running a loop tp print the statisticaly significant variables to be used in logit regression\n",
    "#for name in x_val:\n",
    "   # print(f\"original_df['{name}'] +\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "- The Model below is with statistically significant variables. First I created a model with all the variables and then removed variables that had high p value or p values greater than 0.05. Removal of p values was done in a step by step approach. Removing highest p value variable first and then the next one by one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.409431\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:     CROSS_SELL_SUCCESS   No. Observations:                 1946\n",
      "Model:                          Logit   Df Residuals:                     1936\n",
      "Method:                           MLE   Df Model:                            9\n",
      "Date:                Sun, 15 Mar 2020   Pseudo R-squ.:                  0.3478\n",
      "Time:                        23:50:23   Log-Likelihood:                -796.75\n",
      "converged:                       True   LL-Null:                       -1221.6\n",
      "Covariance Type:            nonrobust   LLR p-value:                4.242e-177\n",
      "==================================================================================================================\n",
      "                                                     coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Intercept                                         -3.5796      0.280    -12.789      0.000      -4.128      -3.031\n",
      "original_df['CANCELLATIONS_BEFORE_NOON']           0.2666      0.045      5.980      0.000       0.179       0.354\n",
      "original_df['FOLLOWED_RECOMMENDATIONS_PCT']        0.0685      0.005     15.218      0.000       0.060       0.077\n",
      "original_df['personal']                            1.1822      0.159      7.434      0.000       0.871       1.494\n",
      "original_df['professional']                        1.8389      0.173     10.649      0.000       1.500       2.177\n",
      "original_df['MOBILE_NUMBER_at']                    0.7976      0.181      4.407      0.000       0.443       1.152\n",
      "original_df['TASTES_AND_PREFERENCES_at']           0.5033      0.136      3.702      0.000       0.237       0.770\n",
      "original_df['PC_LOGINS_at1']                       0.2998      0.124      2.423      0.015       0.057       0.542\n",
      "original_df['MOBILE_LOGINS_at']                    0.3571      0.124      2.891      0.004       0.115       0.599\n",
      "original_df['FOLLOWED_RECOMMENDATIONS_PCT_at']    -1.8183      0.172    -10.576      0.000      -2.155      -1.481\n",
      "==================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "logit_full = smf.logit(formula =  \"\"\"   CROSS_SELL_SUCCESS ~\n",
    "                                        original_df['CANCELLATIONS_BEFORE_NOON'] +\n",
    "                                        original_df['FOLLOWED_RECOMMENDATIONS_PCT'] +\n",
    "                                        original_df['personal']+\n",
    "                                        original_df['professional'] +\n",
    "                                        original_df['MOBILE_NUMBER_at'] +\n",
    "                                        original_df['TASTES_AND_PREFERENCES_at'] +\n",
    "                                        original_df['PC_LOGINS_at1'] +\n",
    "                                        original_df['MOBILE_LOGINS_at'] +\n",
    "                                        original_df['FOLLOWED_RECOMMENDATIONS_PCT_at']\"\"\",\n",
    "                          data = original_df)\n",
    "#Fit the the data\n",
    "logit_full = logit_full.fit()\n",
    "\n",
    "#Display the results\n",
    "print(logit_full.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CREATING DICTIONARY**\n",
    "\n",
    "\n",
    "- The dictionary is created for one will all variables called as \"logit_full\" and the another one with only significant variables called as \"logit_sig\". <br />\n",
    "- This dictionary is created to use these variables and test result on different models and compare model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates the dictionary\n",
    "candidate_dict = {\n",
    "\n",
    "                 #With all the variables significant and non significant\n",
    "                 'logit_full'   : [ 'REVENUE' ,\n",
    "                                    'TOTAL_MEALS_ORDERED' ,\n",
    "                                    'UNIQUE_MEALS_PURCH' ,\n",
    "                                    'CONTACTS_W_CUSTOMER_SERVICE' ,\n",
    "                                    'PRODUCT_CATEGORIES_VIEWED' ,\n",
    "                                    'AVG_TIME_PER_SITE_VISIT' ,\n",
    "                                    'MOBILE_NUMBER' ,\n",
    "                                    'CANCELLATIONS_BEFORE_NOON' ,\n",
    "                                    'CANCELLATIONS_AFTER_NOON' ,\n",
    "                                    'TASTES_AND_PREFERENCES' ,\n",
    "                                    'PC_LOGINS' ,\n",
    "                                    'MOBILE_LOGINS' ,\n",
    "                                    'WEEKLY_PLAN' ,\n",
    "                                    'EARLY_DELIVERIES' ,\n",
    "                                    'LATE_DELIVERIES' ,\n",
    "                                    'PACKAGE_LOCKER' ,\n",
    "                                    'REFRIGERATED_LOCKER' ,\n",
    "                                    'FOLLOWED_RECOMMENDATIONS_PCT' ,\n",
    "                                    'AVG_PREP_VID_TIME' ,\n",
    "                                    'LARGEST_ORDER_SIZE' ,\n",
    "                                    'MASTER_CLASSES_ATTENDED' ,\n",
    "                                    'MEDIAN_MEAL_RATING' ,\n",
    "                                    'AVG_CLICKS_PER_VISIT' ,\n",
    "                                    'TOTAL_PHOTOS_VIEWED' ,\n",
    "                                    'junk' ,\n",
    "                                    'personal' ,\n",
    "                                    'professional' ,\n",
    "                                    'REVENUE_out' ,\n",
    "                                    'TOTAL_MEALS_ORDERED_out' ,\n",
    "                                    'UNIQUE_MEALS_PURCH_out' ,\n",
    "                                    'CONTACTS_W_CUSTOMER_SERVICE_out' ,\n",
    "                                    'PRODUCT_CATEGORIES_VIEWED_at' ,\n",
    "                                    'AVG_TIME_PER_SITE_VISIT_out' ,\n",
    "                                    'MOBILE_NUMBER_at' ,\n",
    "                                    'CANCELLATIONS_BEFORE_NOON_at' ,\n",
    "                                    'CANCELLATIONS_AFTER_NOON_at' ,\n",
    "                                    'TASTES_AND_PREFERENCES_at' ,\n",
    "                                    'PC_LOGINS_at' ,\n",
    "                                    'PC_LOGINS_at1' ,\n",
    "                                    'MOBILE_LOGINS_at' ,\n",
    "                                    'MOBILE_LOGINS_at1' ,\n",
    "                                    'WEEKLY_PLAN_out' ,\n",
    "                                    'EARLY_DELIVERIES_at' ,\n",
    "                                    'LATE_DELIVERIES_at' ,\n",
    "                                    'PACKAGE_LOCKER_at' ,\n",
    "                                    'PACKAGE_LOCKER_at1' ,\n",
    "                                    'REFRIGERATED_LOCKER_at' ,\n",
    "                                    'FOLLOWED_RECOMMENDATIONS_PCT_at' ,\n",
    "                                    'AVG_PREP_VID_TIME_out' ,\n",
    "                                    'LARGEST_ORDER_SIZE_at' ,\n",
    "                                    'MASTER_CLASSES_ATTENDED_at' ,\n",
    "                                    'MASTER_CLASSES_ATTENDED_at1' ,\n",
    "                                    'MEDIAN_MEAL_RATING_at' ,\n",
    "                                    'AVG_CLICKS_PER_VISIT_at' ,\n",
    "                                    'TOTAL_PHOTOS_VIEWED_at'],\n",
    "    \n",
    "              #With statistically significant variables\n",
    "\n",
    "              'logit_sig'     : [   'CANCELLATIONS_BEFORE_NOON',\n",
    "                                    'FOLLOWED_RECOMMENDATIONS_PCT',\n",
    "                                    'personal',\n",
    "                                    'professional',\n",
    "                                    'MOBILE_NUMBER_at',\n",
    "                                    'TASTES_AND_PREFERENCES_at',\n",
    "                                    'PC_LOGINS_at1',\n",
    "                                    'MOBILE_LOGINS_at',\n",
    "                                    'FOLLOWED_RECOMMENDATIONS_PCT_at']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split\n",
    "\n",
    "- Creating train test split with significant variables to see performance on the model. Target variable CROSS_SELL_SUCCESS. Test size as 0.25 that creates a 75% and 25% split between train and test at a given random state of 222.<br />\n",
    "- The train test split with full data set is also created where all the variables are entered, this is done to separate between the significant and full data values. There parameters are same for full and significant variables.<br />\n",
    "- Stratification parameter is also added as to ensure that the train and test sets have approximately the same percentage of samples of each target class as the complete set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split\n",
    "# training data on the x variable that has statisticaly significant values except the tareget variable\n",
    "chef_data   = original_df.loc[ : , candidate_dict['logit_sig']]\n",
    "\n",
    "# target data on the y variable that has the target that we need to achieve\n",
    "# -preparing response variable data - cross sell success\n",
    "chef_target = original_df.loc[:, 'CROSS_SELL_SUCCESS']\n",
    "\n",
    "\n",
    "# test_size = 0.25 and randome_state = 222\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                                    chef_data,\n",
    "                                                    chef_target,\n",
    "                                                    test_size    = 0.25,\n",
    "                                                    random_state = 222,\n",
    "                                                    stratify     = chef_target\n",
    "                                                    )\n",
    "\n",
    "# train/test split with the logit_sig variables\n",
    "chef_data_full   =  original_df.loc[ : , candidate_dict['logit_full']]\n",
    "chef_target_full =  original_df.loc[ : , 'CROSS_SELL_SUCCESS']\n",
    "\n",
    "\n",
    "# train/test split\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(\n",
    "                                                                        chef_data_full,\n",
    "                                                                        chef_target_full,\n",
    "                                                                        random_state = 222,\n",
    "                                                                        test_size    = 0.25,\n",
    "                                                                        stratify     = chef_target\n",
    "                                                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "- Using GridSearchCV for hyperparameter tunning and finding the tuned parameters to for further using in the logistic regression.\n",
    "\n",
    "*Displayed the results of this tune here and commented out the code to increase processing time, the same tuned parameters are used in the regression*<br />\n",
    "\n",
    "**OUTPUT**\n",
    "~~~\n",
    "Tuned Parameters  : {'C': 1.9000000000000001, 'warm_start': True}\n",
    "Tuned CV AUC      : 0.614\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# declaring a hyperparameter space\n",
    "#C_space          = pd.np.arange(0.1, 3.0, 0.1)\n",
    "#warm_start_space = [True, False]\n",
    "\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "#param_grid = {'C'          : C_space,\n",
    "#              'warm_start' : warm_start_space}\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "#lr_tuned = sklearn.linear_model.LogisticRegression(solver = 'lbfgs',\n",
    "#                              max_iter= 1000,\n",
    "#                              random_state = 222)\n",
    "\n",
    "\n",
    "# GridSearchCV object\n",
    "#lr_tuned_cv = GridSearchCV(estimator  = lr_tuned,\n",
    "#                           param_grid = param_grid,\n",
    "#                          cv         = 3,\n",
    "#                           scoring    = make_scorer(roc_auc_score,\n",
    "#                                                    needs_threshold = False))\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "#lr_tuned_cv.fit(chef_data, chef_target)\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "#print(\"Tuned Parameters  :\", lr_tuned_cv.best_params_)\n",
    "#print(\"Tuned CV AUC      :\", lr_tuned_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression Model** with tuned results\n",
    "\n",
    " - Added the results from above code to the model for tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a logistic regression model with tuned values\n",
    "lr_tuned = sklearn.linear_model.LogisticRegression(solver = 'lbfgs',\n",
    "                                                   C = 1.9,\n",
    "                                                   warm_start = True,\n",
    "                                                   max_iter= 1000,\n",
    "                                                   random_state = 222)\n",
    "\n",
    "\n",
    "lr_tuned = lr_tuned.fit(X_train, y_train)\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "lr_tuned_pred = lr_tuned.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating Model Performance** dataframe\n",
    "\n",
    "- For better comparison storing results of each model in a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an empty list\n",
    "model_performance = [['Model', 'Training Accuracy',\n",
    "                      'Testing Accuracy', 'AUC Value']]\n",
    "\n",
    "\n",
    "# train accuracy\n",
    "logreg_train_acc  = lr_tuned.score(X_train, y_train).round(3)\n",
    "\n",
    "\n",
    "# test accuracy\n",
    "logreg_test_acc   = lr_tuned.score(X_test, y_test).round(3)\n",
    "\n",
    "\n",
    "# auc value\n",
    "logreg_auc = roc_auc_score(y_true  = y_test,\n",
    "                           y_score = lr_tuned_pred).round(3)\n",
    "\n",
    "\n",
    "# saving the results\n",
    "model_performance.append(['Tuned Logestic Regression',\n",
    "                          logreg_train_acc,\n",
    "                          logreg_test_acc,\n",
    "                          logreg_auc])\n",
    "\n",
    "#declaring a DataFrame object\n",
    "model_performance = pd.DataFrame(model_performance[1:], columns = model_performance[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier\n",
    "\n",
    "- Using GridSearchCV for hyperparameter tunning and finding the tuned parameters to for further using in the decision tree classifier.\n",
    "\n",
    "*Displayed the results of this tune here and commented out the code to increase processing time, the same tuned parameters are used in the classifier*\n",
    "\n",
    "**OUTPUT**\n",
    "~~~\n",
    "Tuned Parameters  : {'criterion': 'gini', 'max_depth': 8, 'min_samples_leaf': 6, 'splitter': 'best'}\n",
    "Tuned Training AUC: 0.638\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring a hyperparameter space\n",
    "#criterion_space = ['gini', 'entropy']\n",
    "#splitter_space = ['best', 'random']\n",
    "#depth_space = pd.np.arange(1, 25)\n",
    "#leaf_space  = pd.np.arange(1, 100)\n",
    "\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "#param_grid = {'criterion'        : criterion_space,\n",
    "#              'splitter'         : splitter_space,\n",
    "#              'max_depth'        : depth_space,\n",
    "#              'min_samples_leaf' : leaf_space}\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "#tuned_tree = DecisionTreeClassifier(random_state = 222)\n",
    "\n",
    "\n",
    "# GridSearchCV object\n",
    "#tuned_tree_cv = GridSearchCV(estimator  = tuned_tree,\n",
    "#                             param_grid = param_grid,\n",
    " #                            cv         = 3,\n",
    "#                             scoring    = make_scorer(roc_auc_score,\n",
    "#                                                      needs_threshold = False))\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "#tuned_tree_cv.fit(chef_data, chef_target)\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "#print(\"Tuned Parameters  :\", tuned_tree_cv.best_params_)\n",
    "#print(\"Tuned Training AUC:\", tuned_tree_cv.best_score_.round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Tree Classifier** with tuned results\n",
    "\n",
    " - Added the results from above code to the model for tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiating the decision tree classifier\n",
    "tree_tuned =   DecisionTreeClassifier (\n",
    "                                       criterion        = 'gini',\n",
    "                                       max_depth        = 8,\n",
    "                                       min_samples_leaf = 6,\n",
    "                                       splitter         = 'best',\n",
    "                                       random_state     = 222)\n",
    "\n",
    "#fiting the tree on train data\n",
    "tree_tuned = tree_tuned.fit(X_train, y_train)\n",
    "\n",
    "#predicting the tree on the test data\n",
    "tree_tuned_pred = tree_tuned.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the results of decision tree classifier in the model performance data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring model performance objects\n",
    "tree_train_acc = tree_tuned.score(X_train, y_train).round(3)\n",
    "tree_test_acc  = tree_tuned.score(X_test, y_test).round(3)\n",
    "tree_auc       = roc_auc_score(y_true  = y_test,\n",
    "                              y_score = tree_tuned_pred).round(3)\n",
    "\n",
    "\n",
    "# appending to model_performance\n",
    "model_performance = model_performance.append(\n",
    "                          {'Model'             : 'Tuned Tree',\n",
    "                          'Training Accuracy'  : tree_train_acc,\n",
    "                          'Testing Accuracy'   : tree_test_acc,\n",
    "                          'AUC Value'          : tree_auc},\n",
    "                          ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier\n",
    "\n",
    "- Using GridSearchCV for hyperparameter tunning and finding the tuned parameters to for further using in the random forest classifier.\n",
    "\n",
    "*Displayed the results of this tune here and commented out the code to increase processing time, the same tuned parameters are used in the classifier*\n",
    "\n",
    "**OUTPUT**\n",
    "~~~\n",
    "Tuned Parameters  : {'bootstrap': False, 'criterion': 'entropy', 'min_samples_leaf': 1, 'n_estimators': 100, 'warm_start': True}\n",
    "Tuned Training AUC: 0.592\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring a hyperparameter space\n",
    "#estimator_space  = pd.np.arange(100, 1100, 250)\n",
    "#leaf_space       = pd.np.arange(1, 31, 10)\n",
    "#criterion_space  = ['gini', 'entropy']\n",
    "#bootstrap_space  = [True, False]\n",
    "#warm_start_space = [True, False]\n",
    "\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "#param_grid = {'n_estimators'     : estimator_space,\n",
    "#              'min_samples_leaf' : leaf_space,\n",
    "#              'criterion'        : criterion_space,\n",
    "#              'bootstrap'        : bootstrap_space,\n",
    "#              'warm_start'       : warm_start_space}\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "#full_forest_grid = RandomForestClassifier(random_state = 222)\n",
    "\n",
    "\n",
    "# GridSearchCV object\n",
    "#full_forest_cv = GridSearchCV(estimator  = full_forest_grid,\n",
    "#                              param_grid = param_grid,\n",
    "#                              cv         = 3,\n",
    "#                              scoring    = make_scorer(roc_auc_score,\n",
    "#                                           needs_threshold = False))\n",
    "\n",
    "\n",
    "# using full data for cross validation\n",
    "#full_forest_cv.fit(chef_data_full, chef_target_full)\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "#print(\"Tuned Parameters  :\", full_forest_cv.best_params_)\n",
    "#print(\"Tuned Training AUC:\", full_forest_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RANDOM FOREST CLASSIFIER**\n",
    "\n",
    "- Model performance with hyperparameter results\n",
    "\n",
    "- Manually entering tuned parameter to random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Developing model with tuned results\n",
    "full_rf_tuned = RandomForestClassifier(bootstrap        = False,\n",
    "                                       criterion        = 'entropy',\n",
    "                                       min_samples_leaf = 1,\n",
    "                                       n_estimators     = 100,\n",
    "                                       warm_start       = True,\n",
    "                                       random_state     = 222)\n",
    "\n",
    "\n",
    "#Fitting the model on train\n",
    "full_rf_tuned_fit = full_rf_tuned.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#Predicting the result on test data\n",
    "full_rf_tuned_pred = full_rf_tuned_fit.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the results of decision tree classifier in the model performance data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring model performance objects\n",
    "rf_train_acc = full_rf_tuned_fit.score(X_train, y_train).round(3)\n",
    "rf_test_acc  = full_rf_tuned_fit.score(X_test, y_test).round(3)\n",
    "rf_auc       = roc_auc_score(y_true  = y_test,\n",
    "                             y_score = full_rf_tuned_pred).round(3)\n",
    "\n",
    "\n",
    "# appending to model_performance\n",
    "model_performance = model_performance.append(\n",
    "                          {'Model'             : 'Tuned Random Forest',\n",
    "                          'Training Accuracy'  : rf_train_acc,\n",
    "                          'Testing Accuracy'   : rf_test_acc,\n",
    "                          'AUC Value'          : rf_auc},\n",
    "                          ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier\n",
    "\n",
    "This is without using hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING the model object without hyperparameters\n",
    "full_gbm_default = GradientBoostingClassifier(loss          = 'deviance',\n",
    "                                              learning_rate = 0.1,\n",
    "                                              n_estimators  = 100,\n",
    "                                              criterion     = 'friedman_mse',\n",
    "                                              max_depth     = 3,\n",
    "                                              warm_start    = False,\n",
    "                                              random_state  = 222)\n",
    "\n",
    "\n",
    "# FIT step\n",
    "full_gbm_default_fit = full_gbm_default.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "full_gbm_default_pred = full_gbm_default_fit.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the results of decision tree classifier in the model performance data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring model performance objects\n",
    "gbm_train_acc = full_gbm_default_fit.score(X_train, y_train).round(3)\n",
    "gbm_test_acc  = full_gbm_default_fit.score(X_test, y_test).round(3)\n",
    "gbm_auc       = roc_auc_score(y_true  = y_test,\n",
    "                              y_score = full_gbm_default_pred).round(3)\n",
    "\n",
    "\n",
    "# appending to model_performance\n",
    "model_performance = model_performance.append(\n",
    "                          {'Model'             : 'GBM_without_tune',\n",
    "                          'Training Accuracy'  : gbm_train_acc,\n",
    "                          'Testing Accuracy'   : gbm_test_acc,\n",
    "                          'AUC Value'          : gbm_auc},\n",
    "                          ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier\n",
    "\n",
    "- Using GridSearchCV for hyperparameter tunning and finding the tuned parameters to for further using in the gradient boosting classifier.\n",
    "\n",
    "*Displayed the results of this tune here and commented out the code to increase processing time, the same tuned parameters are used in the classifier*\n",
    "\n",
    "**OUTPUT**\n",
    "~~~\n",
    "Tuned Parameters  : {'learning_rate': 1.3000000000000003, 'max_depth': 1, 'n_estimators': 100}\n",
    "Tuned Training AUC: 0.631\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring a hyperparameter space\n",
    "#learn_space     = pd.np.arange(0.1, 1.6, 0.3)\n",
    "#estimator_space = pd.np.arange(50, 250, 50)\n",
    "#depth_space     = pd.np.arange(1, 10)\n",
    "\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "#param_grid = {'learning_rate' : learn_space,\n",
    "#              'max_depth'     : depth_space,\n",
    "#              'n_estimators'  : estimator_space}\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "#full_gbm_grid = GradientBoostingClassifier(random_state = 222)\n",
    "\n",
    "\n",
    "# GridSearchCV object\n",
    "#full_gbm_cv = GridSearchCV(estimator  = full_gbm_grid,\n",
    "#                           param_grid = param_grid,\n",
    "#                           cv         = 3,\n",
    "#                           scoring    = make_scorer(roc_auc_score,\n",
    "#                                        needs_threshold = False))\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "#full_gbm_cv.fit(chef_data_full, chef_target_full)\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "#print(\"Tuned Parameters  :\", full_gbm_cv.best_params_)\n",
    "#print(\"Tuned Training AUC:\", full_gbm_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GRADIENT BOOSTING CLASSIFIER**\n",
    "\n",
    "Model performance with hyperparameter results\n",
    "\n",
    "Manually entering tuned parameter to gradient boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING the model object without hyperparameters\n",
    "gbm_tuned = GradientBoostingClassifier(learning_rate = 1.3,\n",
    "                                       max_depth     = 1,\n",
    "                                       n_estimators  = 100,\n",
    "                                       random_state  = 222)\n",
    "\n",
    "\n",
    "# FIT step is needed as we are not using .best_estimator\n",
    "gbm_tuned_fit = gbm_tuned.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "gbm_tuned_pred = gbm_tuned_fit.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the results of decision tree classifier in the model performance data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring model performance objects\n",
    "gbm_train_acc = gbm_tuned_fit.score(X_train, y_train).round(3)\n",
    "gbm_test_acc  = gbm_tuned_fit.score(X_test, y_test).round(3)\n",
    "gbm_auc       = roc_auc_score(y_true  = y_test,\n",
    "                              y_score = gbm_tuned_pred).round(3)\n",
    "\n",
    "\n",
    "# appending to model_performance\n",
    "model_performance = model_performance.append(\n",
    "                          {'Model'             : 'Tuned GBM',\n",
    "                          'Training Accuracy'  : gbm_train_acc,\n",
    "                          'Testing Accuracy'   : gbm_test_acc,\n",
    "                          'AUC Value'          : gbm_auc},\n",
    "                          ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## **Displaying Model Comparisons**\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Testing Accuracy</th>\n",
       "      <th>AUC Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GBM_without_tune</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tuned GBM</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tuned Tree</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tuned Random Forest</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tuned Logestic Regression</td>\n",
       "      <td>0.791</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Model  Training Accuracy  Testing Accuracy  AUC Value\n",
       "3           GBM_without_tune              0.824             0.797      0.779\n",
       "4                  Tuned GBM              0.812             0.793      0.766\n",
       "1                 Tuned Tree              0.835             0.774      0.758\n",
       "2        Tuned Random Forest              0.897             0.737      0.713\n",
       "0  Tuned Logestic Regression              0.791             0.737      0.703"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_perform = model_performance.sort_values(by = 'AUC Value',\n",
    "                                                  ascending = False)\n",
    "sort_perform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The best model her i got is GBM without tuning will use it as my final model**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
